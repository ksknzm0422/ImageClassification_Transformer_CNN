{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af74bcd-50b6-4b69-80fa-42b25c8eb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71917329-07e1-48fe-a688-20e4388cb1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置多GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90b8073-588d-4661-9a12-04c0ac1ee171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据增强策略，包含CutMix\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(x.size()[0]).to(device)\n",
    "    target_a = y\n",
    "    target_b = y[rand_index]\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
    "    return x, target_a, target_b, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "885be089-b9f0-428a-ad00-1d429729abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载CIFAR-100数据集并进行预处理\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56eef726-47a7-4b59-a3d3-3624a6b54047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平操作\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a272d7-7275-4222-af14-e59c3be4402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x, x, x)[0]\n",
    "        x = self.dropout1(self.norm1(attention + x))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout2(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "# Transformer Classifier\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, embed_size=512, heads=8, dropout=0.1, forward_expansion=4):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(3 * 32 * 32, embed_size)\n",
    "        self.transformer = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "        self.fc = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.embedding(x).unsqueeze(1)  # Add sequence dimension\n",
    "        x = self.transformer(x)\n",
    "        x = self.fc(x.squeeze(1))  # Remove sequence dimension\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ca2f0b-43c7-41dc-9632-e30032f78256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train_model(model, train_loader, test_loader, optimizer, criterion, epochs):\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # 使用CutMix\n",
    "            inputs, targets_a, targets_b, lam = cutmix_data(inputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # 验证模型\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "        writer.add_scalar('Loss/train', running_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss / len(test_loader), epoch)\n",
    "        writer.add_scalar('Accuracy/val', accuracy, epoch)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "446123b1-d213-483e-b291-1fc8ee102f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c949e849-18ec-4155-be0f-0f8ee30e1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型和优化器\n",
    "num_classes = 100\n",
    "\n",
    "model_cnn = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "395e15b6-3cf0-469c-81bb-de08a0497c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 4.581295104026794\n",
      "Epoch 1, Batch 200, Loss: 4.301905860900879\n",
      "Epoch 1, Batch 300, Loss: 4.171725716590881\n",
      "Validation Accuracy: 13.89%\n",
      "Epoch 2, Batch 100, Loss: 4.062727067470551\n",
      "Epoch 2, Batch 200, Loss: 4.010521094799042\n",
      "Epoch 2, Batch 300, Loss: 3.952156367301941\n",
      "Validation Accuracy: 20.73%\n",
      "Epoch 3, Batch 100, Loss: 3.862959442138672\n",
      "Epoch 3, Batch 200, Loss: 3.8389627861976625\n",
      "Epoch 3, Batch 300, Loss: 3.8163853430747987\n",
      "Validation Accuracy: 25.82%\n",
      "Epoch 4, Batch 100, Loss: 3.7059045886993407\n",
      "Epoch 4, Batch 200, Loss: 3.7132679438591003\n",
      "Epoch 4, Batch 300, Loss: 3.690778527259827\n",
      "Validation Accuracy: 30.18%\n",
      "Epoch 5, Batch 100, Loss: 3.621175343990326\n",
      "Epoch 5, Batch 200, Loss: 3.5585072898864745\n",
      "Epoch 5, Batch 300, Loss: 3.5666079473495484\n",
      "Validation Accuracy: 32.08%\n",
      "Epoch 6, Batch 100, Loss: 3.5947267985343934\n",
      "Epoch 6, Batch 200, Loss: 3.4393190932273865\n",
      "Epoch 6, Batch 300, Loss: 3.5309542179107667\n",
      "Validation Accuracy: 35.31%\n",
      "Epoch 7, Batch 100, Loss: 3.480468306541443\n",
      "Epoch 7, Batch 200, Loss: 3.479423339366913\n",
      "Epoch 7, Batch 300, Loss: 3.4455920672416687\n",
      "Validation Accuracy: 36.69%\n",
      "Epoch 8, Batch 100, Loss: 3.4097740936279295\n",
      "Epoch 8, Batch 200, Loss: 3.394318451881409\n",
      "Epoch 8, Batch 300, Loss: 3.3706107258796694\n",
      "Validation Accuracy: 40.58%\n",
      "Epoch 9, Batch 100, Loss: 3.377599241733551\n",
      "Epoch 9, Batch 200, Loss: 3.288646705150604\n",
      "Epoch 9, Batch 300, Loss: 3.3109833002090454\n",
      "Validation Accuracy: 39.12%\n",
      "Epoch 10, Batch 100, Loss: 3.3298548913002013\n",
      "Epoch 10, Batch 200, Loss: 3.2918094396591187\n",
      "Epoch 10, Batch 300, Loss: 3.3104947710037234\n",
      "Validation Accuracy: 42.86%\n",
      "Epoch 11, Batch 100, Loss: 3.305179913043976\n",
      "Epoch 11, Batch 200, Loss: 3.1714111757278443\n",
      "Epoch 11, Batch 300, Loss: 3.1884923696517946\n",
      "Validation Accuracy: 44.35%\n",
      "Epoch 12, Batch 100, Loss: 3.1726145577430724\n",
      "Epoch 12, Batch 200, Loss: 3.1572030878067014\n",
      "Epoch 12, Batch 300, Loss: 3.2254785251617433\n",
      "Validation Accuracy: 43.91%\n",
      "Epoch 13, Batch 100, Loss: 3.189689555168152\n",
      "Epoch 13, Batch 200, Loss: 3.168863413333893\n",
      "Epoch 13, Batch 300, Loss: 3.140158030986786\n",
      "Validation Accuracy: 47.64%\n",
      "Epoch 14, Batch 100, Loss: 3.090147895812988\n",
      "Epoch 14, Batch 200, Loss: 3.12060852766037\n",
      "Epoch 14, Batch 300, Loss: 3.1989951395988463\n",
      "Validation Accuracy: 46.92%\n",
      "Epoch 15, Batch 100, Loss: 3.1252536034584044\n",
      "Epoch 15, Batch 200, Loss: 3.112536075115204\n",
      "Epoch 15, Batch 300, Loss: 3.168343679904938\n",
      "Validation Accuracy: 48.26%\n",
      "Epoch 16, Batch 100, Loss: 3.0620168828964234\n",
      "Epoch 16, Batch 200, Loss: 3.05848509311676\n",
      "Epoch 16, Batch 300, Loss: 3.103767743110657\n",
      "Validation Accuracy: 48.72%\n",
      "Epoch 17, Batch 100, Loss: 3.0386707437038423\n",
      "Epoch 17, Batch 200, Loss: 3.043986817598343\n",
      "Epoch 17, Batch 300, Loss: 3.010590430498123\n",
      "Validation Accuracy: 50.20%\n",
      "Epoch 18, Batch 100, Loss: 3.027340363264084\n",
      "Epoch 18, Batch 200, Loss: 3.051974363327026\n",
      "Epoch 18, Batch 300, Loss: 3.0020642840862273\n",
      "Validation Accuracy: 50.71%\n",
      "Epoch 19, Batch 100, Loss: 3.051412000656128\n",
      "Epoch 19, Batch 200, Loss: 2.937882785797119\n",
      "Epoch 19, Batch 300, Loss: 3.043889966011047\n",
      "Validation Accuracy: 51.31%\n",
      "Epoch 20, Batch 100, Loss: 2.9313443446159364\n",
      "Epoch 20, Batch 200, Loss: 3.0269481492042543\n",
      "Epoch 20, Batch 300, Loss: 3.0046345019340515\n",
      "Validation Accuracy: 51.50%\n",
      "Epoch 21, Batch 100, Loss: 2.967211188077927\n",
      "Epoch 21, Batch 200, Loss: 2.9787973153591154\n",
      "Epoch 21, Batch 300, Loss: 2.8476761317253114\n",
      "Validation Accuracy: 52.64%\n",
      "Epoch 22, Batch 100, Loss: 2.8744072008132933\n",
      "Epoch 22, Batch 200, Loss: 3.0130717742443083\n",
      "Epoch 22, Batch 300, Loss: 2.9570608055591583\n",
      "Validation Accuracy: 51.91%\n",
      "Epoch 23, Batch 100, Loss: 2.9563837027549744\n",
      "Epoch 23, Batch 200, Loss: 2.9323459422588347\n",
      "Epoch 23, Batch 300, Loss: 2.9361013650894163\n",
      "Validation Accuracy: 50.57%\n",
      "Epoch 24, Batch 100, Loss: 2.9508806681632995\n",
      "Epoch 24, Batch 200, Loss: 2.9783199000358582\n",
      "Epoch 24, Batch 300, Loss: 2.883295739889145\n",
      "Validation Accuracy: 54.43%\n",
      "Epoch 25, Batch 100, Loss: 2.9106530964374544\n",
      "Epoch 25, Batch 200, Loss: 2.8991468155384066\n",
      "Epoch 25, Batch 300, Loss: 2.859162254333496\n",
      "Validation Accuracy: 54.62%\n",
      "Epoch 26, Batch 100, Loss: 2.916471984386444\n",
      "Epoch 26, Batch 200, Loss: 2.9559626853466034\n",
      "Epoch 26, Batch 300, Loss: 2.8803367900848387\n",
      "Validation Accuracy: 54.75%\n",
      "Epoch 27, Batch 100, Loss: 2.8998495399951936\n",
      "Epoch 27, Batch 200, Loss: 2.8587206423282625\n",
      "Epoch 27, Batch 300, Loss: 2.921688175201416\n",
      "Validation Accuracy: 55.15%\n",
      "Epoch 28, Batch 100, Loss: 2.928433721065521\n",
      "Epoch 28, Batch 200, Loss: 2.907179913520813\n",
      "Epoch 28, Batch 300, Loss: 2.9284006714820863\n",
      "Validation Accuracy: 55.38%\n",
      "Epoch 29, Batch 100, Loss: 2.922693758010864\n",
      "Epoch 29, Batch 200, Loss: 2.84652862071991\n",
      "Epoch 29, Batch 300, Loss: 2.8735584807395935\n",
      "Validation Accuracy: 55.81%\n",
      "Epoch 30, Batch 100, Loss: 2.819289813041687\n",
      "Epoch 30, Batch 200, Loss: 2.8467521977424624\n",
      "Epoch 30, Batch 300, Loss: 2.818004925251007\n",
      "Validation Accuracy: 55.03%\n",
      "Epoch 31, Batch 100, Loss: 2.7719739818573\n",
      "Epoch 31, Batch 200, Loss: 2.7674597132205965\n",
      "Epoch 31, Batch 300, Loss: 2.805806747674942\n",
      "Validation Accuracy: 55.31%\n",
      "Epoch 32, Batch 100, Loss: 2.865951029062271\n",
      "Epoch 32, Batch 200, Loss: 2.8335370564460756\n",
      "Epoch 32, Batch 300, Loss: 2.8056440234184263\n",
      "Validation Accuracy: 56.36%\n",
      "Epoch 33, Batch 100, Loss: 2.776765296459198\n",
      "Epoch 33, Batch 200, Loss: 2.972168799638748\n",
      "Epoch 33, Batch 300, Loss: 2.967579210996628\n",
      "Validation Accuracy: 56.40%\n",
      "Epoch 34, Batch 100, Loss: 2.7689628279209137\n",
      "Epoch 34, Batch 200, Loss: 2.9023304450511933\n",
      "Epoch 34, Batch 300, Loss: 2.7429758512973788\n",
      "Validation Accuracy: 56.13%\n",
      "Epoch 35, Batch 100, Loss: 2.766014561653137\n",
      "Epoch 35, Batch 200, Loss: 2.797483056783676\n",
      "Epoch 35, Batch 300, Loss: 2.864326312541962\n",
      "Validation Accuracy: 57.21%\n",
      "Epoch 36, Batch 100, Loss: 2.749034037590027\n",
      "Epoch 36, Batch 200, Loss: 2.8335175824165346\n",
      "Epoch 36, Batch 300, Loss: 2.787116360664368\n",
      "Validation Accuracy: 57.54%\n",
      "Epoch 37, Batch 100, Loss: 2.7945856368541717\n",
      "Epoch 37, Batch 200, Loss: 2.75171910405159\n",
      "Epoch 37, Batch 300, Loss: 2.79640256524086\n",
      "Validation Accuracy: 57.34%\n",
      "Epoch 38, Batch 100, Loss: 2.784627742767334\n",
      "Epoch 38, Batch 200, Loss: 2.736717302799225\n",
      "Epoch 38, Batch 300, Loss: 2.7342707335948946\n",
      "Validation Accuracy: 58.02%\n",
      "Epoch 39, Batch 100, Loss: 2.762199912071228\n",
      "Epoch 39, Batch 200, Loss: 2.7441023528575896\n",
      "Epoch 39, Batch 300, Loss: 2.8715746319293975\n",
      "Validation Accuracy: 57.87%\n",
      "Epoch 40, Batch 100, Loss: 2.762970095872879\n",
      "Epoch 40, Batch 200, Loss: 2.6908227360248564\n",
      "Epoch 40, Batch 300, Loss: 2.829045367240906\n",
      "Validation Accuracy: 57.56%\n",
      "Epoch 41, Batch 100, Loss: 2.741739937067032\n",
      "Epoch 41, Batch 200, Loss: 2.6873442697525025\n",
      "Epoch 41, Batch 300, Loss: 2.8345805072784422\n",
      "Validation Accuracy: 58.21%\n",
      "Epoch 42, Batch 100, Loss: 2.7485322046279905\n",
      "Epoch 42, Batch 200, Loss: 2.7491039407253264\n",
      "Epoch 42, Batch 300, Loss: 2.746625701189041\n",
      "Validation Accuracy: 58.04%\n",
      "Epoch 43, Batch 100, Loss: 2.651436047554016\n",
      "Epoch 43, Batch 200, Loss: 2.7404564595222474\n",
      "Epoch 43, Batch 300, Loss: 2.7726300847530365\n",
      "Validation Accuracy: 58.21%\n",
      "Epoch 44, Batch 100, Loss: 2.751794694662094\n",
      "Epoch 44, Batch 200, Loss: 2.734356906414032\n",
      "Epoch 44, Batch 300, Loss: 2.6973882579803465\n",
      "Validation Accuracy: 58.78%\n",
      "Epoch 45, Batch 100, Loss: 2.769452519416809\n",
      "Epoch 45, Batch 200, Loss: 2.7003068113327027\n",
      "Epoch 45, Batch 300, Loss: 2.7539805245399473\n",
      "Validation Accuracy: 59.09%\n",
      "Epoch 46, Batch 100, Loss: 2.7947302508354186\n",
      "Epoch 46, Batch 200, Loss: 2.752109295129776\n",
      "Epoch 46, Batch 300, Loss: 2.7214560663700103\n",
      "Validation Accuracy: 59.26%\n",
      "Epoch 47, Batch 100, Loss: 2.694834326505661\n",
      "Epoch 47, Batch 200, Loss: 2.6515430855751037\n",
      "Epoch 47, Batch 300, Loss: 2.6916123950481414\n",
      "Validation Accuracy: 59.10%\n",
      "Epoch 48, Batch 100, Loss: 2.706899501085281\n",
      "Epoch 48, Batch 200, Loss: 2.758881713151932\n",
      "Epoch 48, Batch 300, Loss: 2.833225272893906\n",
      "Validation Accuracy: 58.48%\n",
      "Epoch 49, Batch 100, Loss: 2.6733534896373747\n",
      "Epoch 49, Batch 200, Loss: 2.8211116659641267\n",
      "Epoch 49, Batch 300, Loss: 2.7350517082214356\n",
      "Validation Accuracy: 59.36%\n",
      "Epoch 50, Batch 100, Loss: 2.701906282901764\n",
      "Epoch 50, Batch 200, Loss: 2.6844535768032074\n",
      "Epoch 50, Batch 300, Loss: 2.6366111862659456\n",
      "Validation Accuracy: 59.13%\n",
      "Epoch 51, Batch 100, Loss: 2.6528781235218046\n",
      "Epoch 51, Batch 200, Loss: 2.6701211404800413\n",
      "Epoch 51, Batch 300, Loss: 2.7541524732112883\n",
      "Validation Accuracy: 59.69%\n",
      "Epoch 52, Batch 100, Loss: 2.653867231607437\n",
      "Epoch 52, Batch 200, Loss: 2.659301117658615\n",
      "Epoch 52, Batch 300, Loss: 2.7137362039089203\n",
      "Validation Accuracy: 58.93%\n",
      "Epoch 53, Batch 100, Loss: 2.628140504360199\n",
      "Epoch 53, Batch 200, Loss: 2.729374945163727\n",
      "Epoch 53, Batch 300, Loss: 2.6865840458869936\n",
      "Validation Accuracy: 59.37%\n",
      "Epoch 54, Batch 100, Loss: 2.678661608695984\n",
      "Epoch 54, Batch 200, Loss: 2.6244851410388947\n",
      "Epoch 54, Batch 300, Loss: 2.7880998182296755\n",
      "Validation Accuracy: 59.42%\n",
      "Epoch 55, Batch 100, Loss: 2.6798390424251557\n",
      "Epoch 55, Batch 200, Loss: 2.7000905549526215\n",
      "Epoch 55, Batch 300, Loss: 2.6963682329654692\n",
      "Validation Accuracy: 60.21%\n",
      "Epoch 56, Batch 100, Loss: 2.7068242704868317\n",
      "Epoch 56, Batch 200, Loss: 2.68861993432045\n",
      "Epoch 56, Batch 300, Loss: 2.7251094484329226\n",
      "Validation Accuracy: 58.88%\n",
      "Epoch 57, Batch 100, Loss: 2.5935025453567504\n",
      "Epoch 57, Batch 200, Loss: 2.6613787734508514\n",
      "Epoch 57, Batch 300, Loss: 2.685514953136444\n",
      "Validation Accuracy: 59.59%\n",
      "Epoch 58, Batch 100, Loss: 2.7254348468780516\n",
      "Epoch 58, Batch 200, Loss: 2.725714690685272\n",
      "Epoch 58, Batch 300, Loss: 2.638359817266464\n",
      "Validation Accuracy: 60.54%\n",
      "Epoch 59, Batch 100, Loss: 2.656786743402481\n",
      "Epoch 59, Batch 200, Loss: 2.621866819858551\n",
      "Epoch 59, Batch 300, Loss: 2.6996983182430268\n",
      "Validation Accuracy: 59.33%\n",
      "Epoch 60, Batch 100, Loss: 2.6209078562259673\n",
      "Epoch 60, Batch 200, Loss: 2.690802496671677\n",
      "Epoch 60, Batch 300, Loss: 2.6525397801399233\n",
      "Validation Accuracy: 60.53%\n",
      "Epoch 61, Batch 100, Loss: 2.62644308924675\n",
      "Epoch 61, Batch 200, Loss: 2.666337239742279\n",
      "Epoch 61, Batch 300, Loss: 2.5453584706783294\n",
      "Validation Accuracy: 60.01%\n",
      "Epoch 62, Batch 100, Loss: 2.696566882133484\n",
      "Epoch 62, Batch 200, Loss: 2.6450139701366426\n",
      "Epoch 62, Batch 300, Loss: 2.66868968129158\n",
      "Validation Accuracy: 60.07%\n",
      "Epoch 63, Batch 100, Loss: 2.5831488931179045\n",
      "Epoch 63, Batch 200, Loss: 2.602005351781845\n",
      "Epoch 63, Batch 300, Loss: 2.537251751422882\n",
      "Validation Accuracy: 61.13%\n",
      "Epoch 64, Batch 100, Loss: 2.7439050137996674\n",
      "Epoch 64, Batch 200, Loss: 2.6540213084220885\n",
      "Epoch 64, Batch 300, Loss: 2.6712367510795594\n",
      "Validation Accuracy: 60.55%\n",
      "Epoch 65, Batch 100, Loss: 2.6358631479740144\n",
      "Epoch 65, Batch 200, Loss: 2.6999521470069885\n",
      "Epoch 65, Batch 300, Loss: 2.665889871120453\n",
      "Validation Accuracy: 60.52%\n",
      "Epoch 66, Batch 100, Loss: 2.6231239426136015\n",
      "Epoch 66, Batch 200, Loss: 2.6604926586151123\n",
      "Epoch 66, Batch 300, Loss: 2.766227741241455\n",
      "Validation Accuracy: 60.28%\n",
      "Epoch 67, Batch 100, Loss: 2.6360523986816404\n",
      "Epoch 67, Batch 200, Loss: 2.6718701696395875\n",
      "Epoch 67, Batch 300, Loss: 2.5983024716377257\n",
      "Validation Accuracy: 60.91%\n",
      "Epoch 68, Batch 100, Loss: 2.7004709780216216\n",
      "Epoch 68, Batch 200, Loss: 2.6612317544221877\n",
      "Epoch 68, Batch 300, Loss: 2.6533976829051973\n",
      "Validation Accuracy: 60.09%\n",
      "Epoch 69, Batch 100, Loss: 2.587085621356964\n",
      "Epoch 69, Batch 200, Loss: 2.615446263551712\n",
      "Epoch 69, Batch 300, Loss: 2.644985522031784\n",
      "Validation Accuracy: 60.58%\n",
      "Epoch 70, Batch 100, Loss: 2.6549820041656496\n",
      "Epoch 70, Batch 200, Loss: 2.7109107065200804\n",
      "Epoch 70, Batch 300, Loss: 2.5771491432189944\n",
      "Validation Accuracy: 60.98%\n",
      "Epoch 71, Batch 100, Loss: 2.6567458295822144\n",
      "Epoch 71, Batch 200, Loss: 2.57917919754982\n",
      "Epoch 71, Batch 300, Loss: 2.653879520893097\n",
      "Validation Accuracy: 60.90%\n",
      "Epoch 72, Batch 100, Loss: 2.598484207391739\n",
      "Epoch 72, Batch 200, Loss: 2.6023059272766114\n",
      "Epoch 72, Batch 300, Loss: 2.6620473158359528\n",
      "Validation Accuracy: 60.74%\n",
      "Epoch 73, Batch 100, Loss: 2.576129468679428\n",
      "Epoch 73, Batch 200, Loss: 2.6223631155490876\n",
      "Epoch 73, Batch 300, Loss: 2.6223939418792725\n",
      "Validation Accuracy: 60.53%\n",
      "Epoch 74, Batch 100, Loss: 2.6247536087036134\n",
      "Epoch 74, Batch 200, Loss: 2.685036849975586\n",
      "Epoch 74, Batch 300, Loss: 2.652332947254181\n",
      "Validation Accuracy: 60.66%\n",
      "Epoch 75, Batch 100, Loss: 2.5908264255523683\n",
      "Epoch 75, Batch 200, Loss: 2.681408294439316\n",
      "Epoch 75, Batch 300, Loss: 2.692342303991318\n",
      "Validation Accuracy: 61.19%\n",
      "Epoch 76, Batch 100, Loss: 2.5888216149806977\n",
      "Epoch 76, Batch 200, Loss: 2.569397234916687\n",
      "Epoch 76, Batch 300, Loss: 2.5663718259334565\n",
      "Validation Accuracy: 60.96%\n",
      "Epoch 77, Batch 100, Loss: 2.582908775806427\n",
      "Epoch 77, Batch 200, Loss: 2.5633389818668366\n",
      "Epoch 77, Batch 300, Loss: 2.6440487933158874\n",
      "Validation Accuracy: 61.10%\n",
      "Epoch 78, Batch 100, Loss: 2.5393419325351716\n",
      "Epoch 78, Batch 200, Loss: 2.6699871754646303\n",
      "Epoch 78, Batch 300, Loss: 2.606942310333252\n",
      "Validation Accuracy: 61.26%\n",
      "Epoch 79, Batch 100, Loss: 2.6517933666706086\n",
      "Epoch 79, Batch 200, Loss: 2.5285189926624296\n",
      "Epoch 79, Batch 300, Loss: 2.641576727628708\n",
      "Validation Accuracy: 60.92%\n",
      "Epoch 80, Batch 100, Loss: 2.60877872467041\n",
      "Epoch 80, Batch 200, Loss: 2.653915983438492\n",
      "Epoch 80, Batch 300, Loss: 2.721704652309418\n",
      "Validation Accuracy: 61.23%\n",
      "Epoch 81, Batch 100, Loss: 2.6289380967617033\n",
      "Epoch 81, Batch 200, Loss: 2.6494531416893006\n",
      "Epoch 81, Batch 300, Loss: 2.556826945543289\n",
      "Validation Accuracy: 60.80%\n",
      "Epoch 82, Batch 100, Loss: 2.5393740344047546\n",
      "Epoch 82, Batch 200, Loss: 2.568093539476395\n",
      "Epoch 82, Batch 300, Loss: 2.628526349067688\n",
      "Validation Accuracy: 60.98%\n",
      "Epoch 83, Batch 100, Loss: 2.5303437232971193\n",
      "Epoch 83, Batch 200, Loss: 2.691418114900589\n",
      "Epoch 83, Batch 300, Loss: 2.4726916539669035\n",
      "Validation Accuracy: 61.13%\n",
      "Epoch 84, Batch 100, Loss: 2.708203641176224\n",
      "Epoch 84, Batch 200, Loss: 2.556460952758789\n",
      "Epoch 84, Batch 300, Loss: 2.5555325627326964\n",
      "Validation Accuracy: 61.61%\n",
      "Epoch 85, Batch 100, Loss: 2.6591335785388948\n",
      "Epoch 85, Batch 200, Loss: 2.599260914325714\n",
      "Epoch 85, Batch 300, Loss: 2.683745436668396\n",
      "Validation Accuracy: 61.58%\n",
      "Epoch 86, Batch 100, Loss: 2.6879934334754942\n",
      "Epoch 86, Batch 200, Loss: 2.5798594832420347\n",
      "Epoch 86, Batch 300, Loss: 2.571248195171356\n",
      "Validation Accuracy: 60.32%\n",
      "Epoch 87, Batch 100, Loss: 2.5608585500717163\n",
      "Epoch 87, Batch 200, Loss: 2.507429586648941\n",
      "Epoch 87, Batch 300, Loss: 2.6246442937850953\n",
      "Validation Accuracy: 61.08%\n",
      "Epoch 88, Batch 100, Loss: 2.6251790022850034\n",
      "Epoch 88, Batch 200, Loss: 2.5466325414180755\n",
      "Epoch 88, Batch 300, Loss: 2.596470617055893\n",
      "Validation Accuracy: 61.86%\n",
      "Epoch 89, Batch 100, Loss: 2.5051244246959685\n",
      "Epoch 89, Batch 200, Loss: 2.6581217205524443\n",
      "Epoch 89, Batch 300, Loss: 2.5672557771205904\n",
      "Validation Accuracy: 61.85%\n",
      "Epoch 90, Batch 100, Loss: 2.521824872493744\n",
      "Epoch 90, Batch 200, Loss: 2.6267223489284515\n",
      "Epoch 90, Batch 300, Loss: 2.5877447628974917\n",
      "Validation Accuracy: 61.13%\n",
      "Epoch 91, Batch 100, Loss: 2.5724320149421693\n",
      "Epoch 91, Batch 200, Loss: 2.5709990310668944\n",
      "Epoch 91, Batch 300, Loss: 2.6526236164569856\n",
      "Validation Accuracy: 62.10%\n",
      "Epoch 92, Batch 100, Loss: 2.5423945844173432\n",
      "Epoch 92, Batch 200, Loss: 2.549087100028992\n",
      "Epoch 92, Batch 300, Loss: 2.566781610250473\n",
      "Validation Accuracy: 61.34%\n",
      "Epoch 93, Batch 100, Loss: 2.523177245855331\n",
      "Epoch 93, Batch 200, Loss: 2.6547398734092713\n",
      "Epoch 93, Batch 300, Loss: 2.470943706035614\n",
      "Validation Accuracy: 60.90%\n",
      "Epoch 94, Batch 100, Loss: 2.5872087824344634\n",
      "Epoch 94, Batch 200, Loss: 2.630905784368515\n",
      "Epoch 94, Batch 300, Loss: 2.546156442165375\n",
      "Validation Accuracy: 61.13%\n",
      "Epoch 95, Batch 100, Loss: 2.630956847071648\n",
      "Epoch 95, Batch 200, Loss: 2.63269248008728\n",
      "Epoch 95, Batch 300, Loss: 2.51590363740921\n",
      "Validation Accuracy: 62.23%\n",
      "Epoch 96, Batch 100, Loss: 2.4808862698078156\n",
      "Epoch 96, Batch 200, Loss: 2.5520587265491486\n",
      "Epoch 96, Batch 300, Loss: 2.5947966313362123\n",
      "Validation Accuracy: 61.39%\n",
      "Epoch 97, Batch 100, Loss: 2.5028832614421845\n",
      "Epoch 97, Batch 200, Loss: 2.6063092648983\n",
      "Epoch 97, Batch 300, Loss: 2.6520000672340394\n",
      "Validation Accuracy: 61.64%\n",
      "Epoch 98, Batch 100, Loss: 2.567393287420273\n",
      "Epoch 98, Batch 200, Loss: 2.5132008361816407\n",
      "Epoch 98, Batch 300, Loss: 2.499001009464264\n",
      "Validation Accuracy: 61.62%\n",
      "Epoch 99, Batch 100, Loss: 2.496975017786026\n",
      "Epoch 99, Batch 200, Loss: 2.576684330701828\n",
      "Epoch 99, Batch 300, Loss: 2.538979091644287\n",
      "Validation Accuracy: 61.76%\n",
      "Epoch 100, Batch 100, Loss: 2.587836710214615\n",
      "Epoch 100, Batch 200, Loss: 2.6030982553958895\n",
      "Epoch 100, Batch 300, Loss: 2.5348973202705385\n",
      "Validation Accuracy: 61.99%\n"
     ]
    }
   ],
   "source": [
    "train_model(model_cnn, train_loader, test_loader, optimizer_cnn, criterion, epochs=100)\n",
    "torch.save(model_cnn.state_dict(), 'cnn_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4690c1c4-18bd-4b1e-918d-0ef18e9e5e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:117: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 4.489048414230346\n",
      "Epoch 1, Batch 200, Loss: 4.374918823242187\n",
      "Epoch 1, Batch 300, Loss: 4.319014465808868\n",
      "Validation Accuracy: 9.31%\n",
      "Epoch 2, Batch 100, Loss: 4.273751482963562\n",
      "Epoch 2, Batch 200, Loss: 4.260625388622284\n",
      "Epoch 2, Batch 300, Loss: 4.237093567848206\n",
      "Validation Accuracy: 10.16%\n",
      "Epoch 3, Batch 100, Loss: 4.218091058731079\n",
      "Epoch 3, Batch 200, Loss: 4.2090130305290225\n",
      "Epoch 3, Batch 300, Loss: 4.203894271850586\n",
      "Validation Accuracy: 10.07%\n",
      "Epoch 4, Batch 100, Loss: 4.169821660518647\n",
      "Epoch 4, Batch 200, Loss: 4.164753770828247\n",
      "Epoch 4, Batch 300, Loss: 4.180019207000733\n",
      "Validation Accuracy: 10.14%\n",
      "Epoch 5, Batch 100, Loss: 4.153359167575836\n",
      "Epoch 5, Batch 200, Loss: 4.139689223766327\n",
      "Epoch 5, Batch 300, Loss: 4.160777134895325\n",
      "Validation Accuracy: 11.03%\n",
      "Epoch 6, Batch 100, Loss: 4.142562098503113\n",
      "Epoch 6, Batch 200, Loss: 4.170066967010498\n",
      "Epoch 6, Batch 300, Loss: 4.142720885276795\n",
      "Validation Accuracy: 11.35%\n",
      "Epoch 7, Batch 100, Loss: 4.096260452270508\n",
      "Epoch 7, Batch 200, Loss: 4.142696306705475\n",
      "Epoch 7, Batch 300, Loss: 4.105013909339905\n",
      "Validation Accuracy: 11.54%\n",
      "Epoch 8, Batch 100, Loss: 4.1404483079910275\n",
      "Epoch 8, Batch 200, Loss: 4.084174718856811\n",
      "Epoch 8, Batch 300, Loss: 4.114226162433624\n",
      "Validation Accuracy: 11.23%\n",
      "Epoch 9, Batch 100, Loss: 4.027803349494934\n",
      "Epoch 9, Batch 200, Loss: 4.068247990608215\n",
      "Epoch 9, Batch 300, Loss: 4.089265515804291\n",
      "Validation Accuracy: 12.08%\n",
      "Epoch 10, Batch 100, Loss: 4.089554121494293\n",
      "Epoch 10, Batch 200, Loss: 4.051025450229645\n",
      "Epoch 10, Batch 300, Loss: 4.0375810670852665\n",
      "Validation Accuracy: 12.05%\n",
      "Epoch 11, Batch 100, Loss: 4.038768920898438\n",
      "Epoch 11, Batch 200, Loss: 4.074183413982391\n",
      "Epoch 11, Batch 300, Loss: 4.04841185092926\n",
      "Validation Accuracy: 12.47%\n",
      "Epoch 12, Batch 100, Loss: 4.032468917369843\n",
      "Epoch 12, Batch 200, Loss: 4.060556981563568\n",
      "Epoch 12, Batch 300, Loss: 4.051997137069702\n",
      "Validation Accuracy: 12.99%\n",
      "Epoch 13, Batch 100, Loss: 3.9886668729782104\n",
      "Epoch 13, Batch 200, Loss: 4.028376514911652\n",
      "Epoch 13, Batch 300, Loss: 4.012307393550873\n",
      "Validation Accuracy: 12.60%\n",
      "Epoch 14, Batch 100, Loss: 4.00165155172348\n",
      "Epoch 14, Batch 200, Loss: 4.034865241050721\n",
      "Epoch 14, Batch 300, Loss: 4.026763234138489\n",
      "Validation Accuracy: 12.64%\n",
      "Epoch 15, Batch 100, Loss: 4.034677081108093\n",
      "Epoch 15, Batch 200, Loss: 4.024630007743835\n",
      "Epoch 15, Batch 300, Loss: 4.001103551387787\n",
      "Validation Accuracy: 12.65%\n",
      "Epoch 16, Batch 100, Loss: 3.98619735956192\n",
      "Epoch 16, Batch 200, Loss: 3.9873952198028566\n",
      "Epoch 16, Batch 300, Loss: 4.041291706562042\n",
      "Validation Accuracy: 12.61%\n",
      "Epoch 17, Batch 100, Loss: 4.026350231170654\n",
      "Epoch 17, Batch 200, Loss: 3.9832180070877077\n",
      "Epoch 17, Batch 300, Loss: 3.9676018166542053\n",
      "Validation Accuracy: 13.47%\n",
      "Epoch 18, Batch 100, Loss: 3.9537891936302185\n",
      "Epoch 18, Batch 200, Loss: 3.9700910568237306\n",
      "Epoch 18, Batch 300, Loss: 3.976648972034454\n",
      "Validation Accuracy: 13.71%\n",
      "Epoch 19, Batch 100, Loss: 3.957676560878754\n",
      "Epoch 19, Batch 200, Loss: 4.007738575935364\n",
      "Epoch 19, Batch 300, Loss: 3.985250380039215\n",
      "Validation Accuracy: 13.65%\n",
      "Epoch 20, Batch 100, Loss: 3.9957474160194395\n",
      "Epoch 20, Batch 200, Loss: 3.9417174220085145\n",
      "Epoch 20, Batch 300, Loss: 4.002738788127899\n",
      "Validation Accuracy: 13.86%\n",
      "Epoch 21, Batch 100, Loss: 3.9527253341674804\n",
      "Epoch 21, Batch 200, Loss: 3.976270179748535\n",
      "Epoch 21, Batch 300, Loss: 3.950186131000519\n",
      "Validation Accuracy: 14.25%\n",
      "Epoch 22, Batch 100, Loss: 3.9932597851753235\n",
      "Epoch 22, Batch 200, Loss: 3.938611669540405\n",
      "Epoch 22, Batch 300, Loss: 3.9333138251304627\n",
      "Validation Accuracy: 13.85%\n",
      "Epoch 23, Batch 100, Loss: 3.9024017381668092\n",
      "Epoch 23, Batch 200, Loss: 3.939367609024048\n",
      "Epoch 23, Batch 300, Loss: 3.969944221973419\n",
      "Validation Accuracy: 14.67%\n",
      "Epoch 24, Batch 100, Loss: 3.9045387053489686\n",
      "Epoch 24, Batch 200, Loss: 3.9482376384735107\n",
      "Epoch 24, Batch 300, Loss: 3.9846608805656434\n",
      "Validation Accuracy: 14.56%\n",
      "Epoch 25, Batch 100, Loss: 3.9424442219734193\n",
      "Epoch 25, Batch 200, Loss: 3.9579302883148193\n",
      "Epoch 25, Batch 300, Loss: 3.912012414932251\n",
      "Validation Accuracy: 14.14%\n",
      "Epoch 26, Batch 100, Loss: 3.8755643248558043\n",
      "Epoch 26, Batch 200, Loss: 3.9225389170646667\n",
      "Epoch 26, Batch 300, Loss: 3.9377939605712893\n",
      "Validation Accuracy: 15.66%\n",
      "Epoch 27, Batch 100, Loss: 3.8909154081344606\n",
      "Epoch 27, Batch 200, Loss: 3.9607757329940796\n",
      "Epoch 27, Batch 300, Loss: 3.9126942133903504\n",
      "Validation Accuracy: 14.98%\n",
      "Epoch 28, Batch 100, Loss: 3.9301420545578\n",
      "Epoch 28, Batch 200, Loss: 3.900369129180908\n",
      "Epoch 28, Batch 300, Loss: 3.957183134555817\n",
      "Validation Accuracy: 14.37%\n",
      "Epoch 29, Batch 100, Loss: 3.9207864594459534\n",
      "Epoch 29, Batch 200, Loss: 3.963040125370026\n",
      "Epoch 29, Batch 300, Loss: 3.886472682952881\n",
      "Validation Accuracy: 15.75%\n",
      "Epoch 30, Batch 100, Loss: 3.932901611328125\n",
      "Epoch 30, Batch 200, Loss: 3.9282658100128174\n",
      "Epoch 30, Batch 300, Loss: 3.921244282722473\n",
      "Validation Accuracy: 14.95%\n",
      "Epoch 31, Batch 100, Loss: 3.8721426343917846\n",
      "Epoch 31, Batch 200, Loss: 3.93070823431015\n",
      "Epoch 31, Batch 300, Loss: 3.9152049350738527\n",
      "Validation Accuracy: 16.40%\n",
      "Epoch 32, Batch 100, Loss: 3.8854488039016726\n",
      "Epoch 32, Batch 200, Loss: 3.90312655210495\n",
      "Epoch 32, Batch 300, Loss: 3.903394420146942\n",
      "Validation Accuracy: 15.21%\n",
      "Epoch 33, Batch 100, Loss: 3.8859440016746523\n",
      "Epoch 33, Batch 200, Loss: 3.8666799449920655\n",
      "Epoch 33, Batch 300, Loss: 3.9477708601951598\n",
      "Validation Accuracy: 15.55%\n",
      "Epoch 34, Batch 100, Loss: 3.9002436113357546\n",
      "Epoch 34, Batch 200, Loss: 3.8809178066253662\n",
      "Epoch 34, Batch 300, Loss: 3.847233438491821\n",
      "Validation Accuracy: 16.36%\n",
      "Epoch 35, Batch 100, Loss: 3.8963560581207277\n",
      "Epoch 35, Batch 200, Loss: 3.9190309953689577\n",
      "Epoch 35, Batch 300, Loss: 3.874689192771912\n",
      "Validation Accuracy: 16.41%\n",
      "Epoch 36, Batch 100, Loss: 3.93094934463501\n",
      "Epoch 36, Batch 200, Loss: 3.8837902426719664\n",
      "Epoch 36, Batch 300, Loss: 3.928206198215485\n",
      "Validation Accuracy: 16.01%\n",
      "Epoch 37, Batch 100, Loss: 3.905397319793701\n",
      "Epoch 37, Batch 200, Loss: 3.8730444502830506\n",
      "Epoch 37, Batch 300, Loss: 3.88925345659256\n",
      "Validation Accuracy: 15.40%\n",
      "Epoch 38, Batch 100, Loss: 3.854853482246399\n",
      "Epoch 38, Batch 200, Loss: 3.896536657810211\n",
      "Epoch 38, Batch 300, Loss: 3.892027106285095\n",
      "Validation Accuracy: 15.01%\n",
      "Epoch 39, Batch 100, Loss: 3.8584528088569643\n",
      "Epoch 39, Batch 200, Loss: 3.8759338545799253\n",
      "Epoch 39, Batch 300, Loss: 3.8892355823516844\n",
      "Validation Accuracy: 16.41%\n",
      "Epoch 40, Batch 100, Loss: 3.902140185832977\n",
      "Epoch 40, Batch 200, Loss: 3.847497386932373\n",
      "Epoch 40, Batch 300, Loss: 3.9097814059257505\n",
      "Validation Accuracy: 15.68%\n",
      "Epoch 41, Batch 100, Loss: 3.899630057811737\n",
      "Epoch 41, Batch 200, Loss: 3.891321165561676\n",
      "Epoch 41, Batch 300, Loss: 3.872391526699066\n",
      "Validation Accuracy: 16.40%\n",
      "Epoch 42, Batch 100, Loss: 3.8614211177825926\n",
      "Epoch 42, Batch 200, Loss: 3.887345058917999\n",
      "Epoch 42, Batch 300, Loss: 3.9071388936042784\n",
      "Validation Accuracy: 16.69%\n",
      "Epoch 43, Batch 100, Loss: 3.812744634151459\n",
      "Epoch 43, Batch 200, Loss: 3.87747727394104\n",
      "Epoch 43, Batch 300, Loss: 3.832311418056488\n",
      "Validation Accuracy: 16.92%\n",
      "Epoch 44, Batch 100, Loss: 3.872216408252716\n",
      "Epoch 44, Batch 200, Loss: 3.8413020753860474\n",
      "Epoch 44, Batch 300, Loss: 3.870310170650482\n",
      "Validation Accuracy: 16.04%\n",
      "Epoch 45, Batch 100, Loss: 3.8565024018287657\n",
      "Epoch 45, Batch 200, Loss: 3.8882256031036375\n",
      "Epoch 45, Batch 300, Loss: 3.84032897233963\n",
      "Validation Accuracy: 16.92%\n",
      "Epoch 46, Batch 100, Loss: 3.8990996885299682\n",
      "Epoch 46, Batch 200, Loss: 3.820996642112732\n",
      "Epoch 46, Batch 300, Loss: 3.85009272813797\n",
      "Validation Accuracy: 16.81%\n",
      "Epoch 47, Batch 100, Loss: 3.822675397396088\n",
      "Epoch 47, Batch 200, Loss: 3.838834083080292\n",
      "Epoch 47, Batch 300, Loss: 3.866241147518158\n",
      "Validation Accuracy: 16.59%\n",
      "Epoch 48, Batch 100, Loss: 3.8395543909072876\n",
      "Epoch 48, Batch 200, Loss: 3.8703715777397156\n",
      "Epoch 48, Batch 300, Loss: 3.8520264291763304\n",
      "Validation Accuracy: 16.64%\n",
      "Epoch 49, Batch 100, Loss: 3.831181857585907\n",
      "Epoch 49, Batch 200, Loss: 3.859074811935425\n",
      "Epoch 49, Batch 300, Loss: 3.893011226654053\n",
      "Validation Accuracy: 16.62%\n",
      "Epoch 50, Batch 100, Loss: 3.8422256088256836\n",
      "Epoch 50, Batch 200, Loss: 3.886193175315857\n",
      "Epoch 50, Batch 300, Loss: 3.844779224395752\n",
      "Validation Accuracy: 16.70%\n",
      "Epoch 51, Batch 100, Loss: 3.8755309867858885\n",
      "Epoch 51, Batch 200, Loss: 3.9383757972717284\n",
      "Epoch 51, Batch 300, Loss: 3.8194461703300475\n",
      "Validation Accuracy: 16.60%\n",
      "Epoch 52, Batch 100, Loss: 3.8713476157188413\n",
      "Epoch 52, Batch 200, Loss: 3.8408912229537964\n",
      "Epoch 52, Batch 300, Loss: 3.875206787586212\n",
      "Validation Accuracy: 17.53%\n",
      "Epoch 53, Batch 100, Loss: 3.882591259479523\n",
      "Epoch 53, Batch 200, Loss: 3.884789741039276\n",
      "Epoch 53, Batch 300, Loss: 3.827473521232605\n",
      "Validation Accuracy: 18.08%\n",
      "Epoch 54, Batch 100, Loss: 3.8084423327445984\n",
      "Epoch 54, Batch 200, Loss: 3.8298372650146484\n",
      "Epoch 54, Batch 300, Loss: 3.8343482398986817\n",
      "Validation Accuracy: 16.91%\n",
      "Epoch 55, Batch 100, Loss: 3.8861914467811585\n",
      "Epoch 55, Batch 200, Loss: 3.787786827087402\n",
      "Epoch 55, Batch 300, Loss: 3.8602674269676207\n",
      "Validation Accuracy: 18.08%\n",
      "Epoch 56, Batch 100, Loss: 3.8044816064834595\n",
      "Epoch 56, Batch 200, Loss: 3.8473824763298037\n",
      "Epoch 56, Batch 300, Loss: 3.9005647683143616\n",
      "Validation Accuracy: 17.60%\n",
      "Epoch 57, Batch 100, Loss: 3.835685184001923\n",
      "Epoch 57, Batch 200, Loss: 3.8506328082084655\n",
      "Epoch 57, Batch 300, Loss: 3.8212041640281678\n",
      "Validation Accuracy: 17.58%\n",
      "Epoch 58, Batch 100, Loss: 3.7866442728042604\n",
      "Epoch 58, Batch 200, Loss: 3.875257875919342\n",
      "Epoch 58, Batch 300, Loss: 3.852804045677185\n",
      "Validation Accuracy: 18.09%\n",
      "Epoch 59, Batch 100, Loss: 3.754376566410065\n",
      "Epoch 59, Batch 200, Loss: 3.757199511528015\n",
      "Epoch 59, Batch 300, Loss: 3.846988914012909\n",
      "Validation Accuracy: 17.85%\n",
      "Epoch 60, Batch 100, Loss: 3.8251993751525877\n",
      "Epoch 60, Batch 200, Loss: 3.814924461841583\n",
      "Epoch 60, Batch 300, Loss: 3.793375790119171\n",
      "Validation Accuracy: 18.30%\n",
      "Epoch 61, Batch 100, Loss: 3.8410769367218016\n",
      "Epoch 61, Batch 200, Loss: 3.773503532409668\n",
      "Epoch 61, Batch 300, Loss: 3.83467511177063\n",
      "Validation Accuracy: 18.58%\n",
      "Epoch 62, Batch 100, Loss: 3.8494512915611265\n",
      "Epoch 62, Batch 200, Loss: 3.7743341016769407\n",
      "Epoch 62, Batch 300, Loss: 3.7968270778656006\n",
      "Validation Accuracy: 18.35%\n",
      "Epoch 63, Batch 100, Loss: 3.8307856464385988\n",
      "Epoch 63, Batch 200, Loss: 3.8007422041893006\n",
      "Epoch 63, Batch 300, Loss: 3.8023596715927126\n",
      "Validation Accuracy: 17.91%\n",
      "Epoch 64, Batch 100, Loss: 3.779489459991455\n",
      "Epoch 64, Batch 200, Loss: 3.8090096712112427\n",
      "Epoch 64, Batch 300, Loss: 3.757585518360138\n",
      "Validation Accuracy: 18.05%\n",
      "Epoch 65, Batch 100, Loss: 3.8163523411750795\n",
      "Epoch 65, Batch 200, Loss: 3.806462209224701\n",
      "Epoch 65, Batch 300, Loss: 3.8230042314529418\n",
      "Validation Accuracy: 18.80%\n",
      "Epoch 66, Batch 100, Loss: 3.8023840260505675\n",
      "Epoch 66, Batch 200, Loss: 3.782824442386627\n",
      "Epoch 66, Batch 300, Loss: 3.786333737373352\n",
      "Validation Accuracy: 18.52%\n",
      "Epoch 67, Batch 100, Loss: 3.7929071092605593\n",
      "Epoch 67, Batch 200, Loss: 3.804991638660431\n",
      "Epoch 67, Batch 300, Loss: 3.7882141041755677\n",
      "Validation Accuracy: 18.85%\n",
      "Epoch 68, Batch 100, Loss: 3.7321356534957886\n",
      "Epoch 68, Batch 200, Loss: 3.820055251121521\n",
      "Epoch 68, Batch 300, Loss: 3.8112624382972715\n",
      "Validation Accuracy: 19.65%\n",
      "Epoch 69, Batch 100, Loss: 3.8080702447891235\n",
      "Epoch 69, Batch 200, Loss: 3.7856073546409608\n",
      "Epoch 69, Batch 300, Loss: 3.787545862197876\n",
      "Validation Accuracy: 18.05%\n",
      "Epoch 70, Batch 100, Loss: 3.7565807461738587\n",
      "Epoch 70, Batch 200, Loss: 3.7835119128227235\n",
      "Epoch 70, Batch 300, Loss: 3.7217121744155883\n",
      "Validation Accuracy: 19.05%\n",
      "Epoch 71, Batch 100, Loss: 3.7853610467910768\n",
      "Epoch 71, Batch 200, Loss: 3.806460452079773\n",
      "Epoch 71, Batch 300, Loss: 3.8454284930229186\n",
      "Validation Accuracy: 19.02%\n",
      "Epoch 72, Batch 100, Loss: 3.75983952999115\n",
      "Epoch 72, Batch 200, Loss: 3.8007907390594484\n",
      "Epoch 72, Batch 300, Loss: 3.7660523056983948\n",
      "Validation Accuracy: 20.18%\n",
      "Epoch 73, Batch 100, Loss: 3.794609098434448\n",
      "Epoch 73, Batch 200, Loss: 3.798799750804901\n",
      "Epoch 73, Batch 300, Loss: 3.760793220996857\n",
      "Validation Accuracy: 19.73%\n",
      "Epoch 74, Batch 100, Loss: 3.82019948720932\n",
      "Epoch 74, Batch 200, Loss: 3.7496190595626833\n",
      "Epoch 74, Batch 300, Loss: 3.800844097137451\n",
      "Validation Accuracy: 20.18%\n",
      "Epoch 75, Batch 100, Loss: 3.7559310674667357\n",
      "Epoch 75, Batch 200, Loss: 3.7593619656562804\n",
      "Epoch 75, Batch 300, Loss: 3.8244300532341002\n",
      "Validation Accuracy: 19.33%\n",
      "Epoch 76, Batch 100, Loss: 3.805945451259613\n",
      "Epoch 76, Batch 200, Loss: 3.813427700996399\n",
      "Epoch 76, Batch 300, Loss: 3.8440906596183777\n",
      "Validation Accuracy: 19.62%\n",
      "Epoch 77, Batch 100, Loss: 3.7678002882003785\n",
      "Epoch 77, Batch 200, Loss: 3.7590387940406798\n",
      "Epoch 77, Batch 300, Loss: 3.793184747695923\n",
      "Validation Accuracy: 19.04%\n",
      "Epoch 78, Batch 100, Loss: 3.7826987433433534\n",
      "Epoch 78, Batch 200, Loss: 3.7792463278770447\n",
      "Epoch 78, Batch 300, Loss: 3.7817313933372496\n",
      "Validation Accuracy: 18.78%\n",
      "Epoch 79, Batch 100, Loss: 3.801697828769684\n",
      "Epoch 79, Batch 200, Loss: 3.7837307953834536\n",
      "Epoch 79, Batch 300, Loss: 3.8187638545036315\n",
      "Validation Accuracy: 17.99%\n",
      "Epoch 80, Batch 100, Loss: 3.7519345903396606\n",
      "Epoch 80, Batch 200, Loss: 3.7698404884338377\n",
      "Epoch 80, Batch 300, Loss: 3.7800149369239806\n",
      "Validation Accuracy: 18.73%\n",
      "Epoch 81, Batch 100, Loss: 3.8158182549476622\n",
      "Epoch 81, Batch 200, Loss: 3.7230911922454832\n",
      "Epoch 81, Batch 300, Loss: 3.7954028916358946\n",
      "Validation Accuracy: 20.14%\n",
      "Epoch 82, Batch 100, Loss: 3.810119352340698\n",
      "Epoch 82, Batch 200, Loss: 3.728435544967651\n",
      "Epoch 82, Batch 300, Loss: 3.770520565509796\n",
      "Validation Accuracy: 19.34%\n",
      "Epoch 83, Batch 100, Loss: 3.75761146068573\n",
      "Epoch 83, Batch 200, Loss: 3.821412718296051\n",
      "Epoch 83, Batch 300, Loss: 3.785654504299164\n",
      "Validation Accuracy: 18.88%\n",
      "Epoch 84, Batch 100, Loss: 3.759629271030426\n",
      "Epoch 84, Batch 200, Loss: 3.7476121091842653\n",
      "Epoch 84, Batch 300, Loss: 3.7620756602287293\n",
      "Validation Accuracy: 19.45%\n",
      "Epoch 85, Batch 100, Loss: 3.735908045768738\n",
      "Epoch 85, Batch 200, Loss: 3.7799523735046385\n",
      "Epoch 85, Batch 300, Loss: 3.777824673652649\n",
      "Validation Accuracy: 19.93%\n",
      "Epoch 86, Batch 100, Loss: 3.764290943145752\n",
      "Epoch 86, Batch 200, Loss: 3.7220054388046266\n",
      "Epoch 86, Batch 300, Loss: 3.722853572368622\n",
      "Validation Accuracy: 20.10%\n",
      "Epoch 87, Batch 100, Loss: 3.7353503203392027\n",
      "Epoch 87, Batch 200, Loss: 3.756814181804657\n",
      "Epoch 87, Batch 300, Loss: 3.814522957801819\n",
      "Validation Accuracy: 19.63%\n",
      "Epoch 88, Batch 100, Loss: 3.7275980281829835\n",
      "Epoch 88, Batch 200, Loss: 3.728910381793976\n",
      "Epoch 88, Batch 300, Loss: 3.751507556438446\n",
      "Validation Accuracy: 19.60%\n",
      "Epoch 89, Batch 100, Loss: 3.8104035210609437\n",
      "Epoch 89, Batch 200, Loss: 3.7537318110466003\n",
      "Epoch 89, Batch 300, Loss: 3.822020790576935\n",
      "Validation Accuracy: 20.55%\n",
      "Epoch 90, Batch 100, Loss: 3.728084464073181\n",
      "Epoch 90, Batch 200, Loss: 3.7608688569068907\n",
      "Epoch 90, Batch 300, Loss: 3.788477544784546\n",
      "Validation Accuracy: 19.59%\n",
      "Epoch 91, Batch 100, Loss: 3.7995974683761595\n",
      "Epoch 91, Batch 200, Loss: 3.6942813110351564\n",
      "Epoch 91, Batch 300, Loss: 3.8109941339492797\n",
      "Validation Accuracy: 20.42%\n",
      "Epoch 92, Batch 100, Loss: 3.744936089515686\n",
      "Epoch 92, Batch 200, Loss: 3.763277986049652\n",
      "Epoch 92, Batch 300, Loss: 3.687012777328491\n",
      "Validation Accuracy: 20.50%\n",
      "Epoch 93, Batch 100, Loss: 3.7613553333282472\n",
      "Epoch 93, Batch 200, Loss: 3.8187005162239074\n",
      "Epoch 93, Batch 300, Loss: 3.787319142818451\n",
      "Validation Accuracy: 19.68%\n",
      "Epoch 94, Batch 100, Loss: 3.7643103218078613\n",
      "Epoch 94, Batch 200, Loss: 3.794116110801697\n",
      "Epoch 94, Batch 300, Loss: 3.7451618671417237\n",
      "Validation Accuracy: 20.69%\n",
      "Epoch 95, Batch 100, Loss: 3.8021565961837767\n",
      "Epoch 95, Batch 200, Loss: 3.715357482433319\n",
      "Epoch 95, Batch 300, Loss: 3.792701172828674\n",
      "Validation Accuracy: 20.82%\n",
      "Epoch 96, Batch 100, Loss: 3.8173811984062196\n",
      "Epoch 96, Batch 200, Loss: 3.786516990661621\n",
      "Epoch 96, Batch 300, Loss: 3.797586498260498\n",
      "Validation Accuracy: 20.54%\n",
      "Epoch 97, Batch 100, Loss: 3.7916583752632143\n",
      "Epoch 97, Batch 200, Loss: 3.7775636553764342\n",
      "Epoch 97, Batch 300, Loss: 3.7631124472618103\n",
      "Validation Accuracy: 21.03%\n",
      "Epoch 98, Batch 100, Loss: 3.727795901298523\n",
      "Epoch 98, Batch 200, Loss: 3.7403100609779356\n",
      "Epoch 98, Batch 300, Loss: 3.7208752250671386\n",
      "Validation Accuracy: 20.66%\n",
      "Epoch 99, Batch 100, Loss: 3.738661332130432\n",
      "Epoch 99, Batch 200, Loss: 3.7635650968551637\n",
      "Epoch 99, Batch 300, Loss: 3.793637070655823\n",
      "Validation Accuracy: 21.02%\n",
      "Epoch 100, Batch 100, Loss: 3.6896007513999938\n",
      "Epoch 100, Batch 200, Loss: 3.7268880152702333\n",
      "Epoch 100, Batch 300, Loss: 3.758060212135315\n",
      "Validation Accuracy: 20.43%\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model_transformer = TransformerClassifier(num_classes)\n",
    "# 训练参数\n",
    "optimizer_transformer = optim.Adam(model_transformer.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_model(model_transformer, train_loader, test_loader, optimizer_transformer, criterion, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab9866da-42aa-476d-b772-ba59c65adfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存权重\n",
    "torch.save(model_transformer.state_dict(), 'transformer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
